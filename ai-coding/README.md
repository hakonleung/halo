# ARC

AI Coding 工作流：Audit, Auto, Revert, Code。

面向需求 → 设计 → 实现 → 验证 → 部署的全链路 AI 协作流程，强调可审计、可回滚与质量门槛。

## 使用简介

**初始化**
- 新项目：`/flow-create`
- 既有项目：`/flow-init`

**执行**
- 完整流程（7 阶段）：`/flow-start [需求]`
- 轻量流程（3 阶段）：`/flow-quick [需求]`
- 迭代：`/flow-iterate [PRD_XXX] [需求]`
- 重构：`/flow-refactor [范围] [目标]`

**状态与恢复**
- ` /flow-status` / `/flow-list` / `/flow-continue`
- ` /flow-sync-check`（同步率检查）

**脚本与 CI**
- 同步检查：`./ai-coding/scripts/flow-sync-check.sh`
- 阶段验证：`./ai-coding/scripts/flow-stage-validate.sh --stage all`
- CI 示例：`.github/workflows/ai-coding-quality.yml`

## 原理简析

1. **分阶段与风险闸门**：将需求、PRD、UI、技术设计、实现、验证、部署拆分为显式阶段，并在高风险阶段强制人类验收。
2. **文档驱动与交叉验证**：以阶段产出文档为中枢，配合反向推导/数据流模拟等交叉验证，减少理解偏差。
3. **测试左移**：在技术设计阶段草拟测试计划，并在实现阶段同步写测试，降低回归成本。
4. **文档-代码同步率**：对 API、类型、UI 组件建立同步门槛，防止“文档漂移”。
5. **上下文压缩与快照**：通过 `summary.md` 与快照机制保障长流程可持续与可恢复。

## 横向对比（估算）

> 说明：以下为 **中等复杂度需求** 的粗略估算（例如 3-5 个接口、2-4 个页面、前后端联动）。  
> 实际结果取决于项目规模、已有资产、团队熟练度与自动化程度。

### 关键指标对比

> 评分为相对评价，5星最好，可用半星；表内空位不显示，列头总体评分保留空位为黑（token/耗时/负担/人类介入程度为越低越好）。价格按 $10/百万 token 粗略估算，可按模型调整。

| 指标 | ARC<br>🌕🌕🌕🌕🌗 | ARC quick<br>🌕🌕🌕🌕🌑 | OpenSpec<br>🌕🌕🌕🌗🌑 | AutoGen<br>🌕🌕🌕🌗🌑 | TDD<br>🌕🌕🌕🌗🌑 | Prompt<br>🌕🌕🌗🌑🌑 |
|---|---|---|---|---|---|---|
| 学习成本 | 🌕🌕🌕🌗<br>学习曲线陡但一旦掌握复用性高<br>后续需求复用模板与脚本降负担 | 🌕🌕🌕🌗<br>规则与目录一致，学习成本接近 ARC<br>但文档量更少 | 🌕🌕🌕🌗<br>规范化上手中等<br>后续可复用但实现约束有限 | 🌕🌕🌗<br>配置与协作成本高<br>成熟后可部分复用 | 🌕🌕🌕🌗<br>需建立测试优先心智<br>后续修改成本降低 | 🌕🌕🌕🌕🌗<br>上手最快<br>但后续需求缺乏复用沉淀 |
| 可行性 | 🌕🌕🌕🌕🌗<br>需求→PRD→设计→实现全链路验收<br>同步率门槛+验证脚本 | 🌕🌕🌕🌕<br>省略部分设计阶段<br>仍保留验证与同步检查 | 🌕🌕🌕🌕<br>规范清晰降低歧义<br>实现期约束相对弱 | 🌕🌕🌕🌕<br>多代理互补覆盖面广<br>集成稳定性需控制 | 🌕🌕🌕🌗<br>测试驱动约束需求落地<br>复杂场景易欠测 | 🌕🌕🌕<br>对话驱动缺少阶段校验<br>依赖个人补边界 |
| 质量 | 🌕🌕🌕🌕🌗<br>测试左移+阶段验证<br>文档/代码同步检查 | 🌕🌕🌕🌕<br>测试与验证保留<br>文档粒度减少 | 🌕🌕🌕🌕<br>结构化设计保障一致性<br>测试门槛不固定 | 🌕🌕🌕🌗<br>多代理评审与改写<br>质量依赖提示与工具 | 🌕🌕🌕🌕<br>测试优先提升稳健性<br>覆盖度决定上限 | 🌕🌕🌗<br>测试覆盖不稳定<br>缺少统一门槛 |
| 可控性 | 🌕🌕🌕🌕🌗<br>阶段闸门+人类验收<br>范围/风险有硬门槛 | 🌕🌕🌕🌕<br>闸门减少但仍存在<br>范围控制优于 Prompt | 🌕🌕🌕<br>依赖规范执行力度<br>缺少强制同步门槛 | 🌕🌕🌕<br>多代理并行带来漂移风险<br>需强约束策略 | 🌕🌕🌕<br>测试约束功能边界<br>需求范围控制有限 | 🌕🌕<br>范围易漂移<br>返工难预测 |
| 可回溯 | 🌕🌕🌕🌕🌗<br>history.json+快照记录<br>summary 索引可追踪 | 🌕🌕🌕🌕<br>保留主要记录与摘要<br>阶段更少但仍可追踪 | 🌕🌕🌕<br>依赖规范版本管理<br>实现细节回溯弱 | 🌕🌕🌕<br>日志可追踪但分散<br>规范化程度不一 | 🌕🌕🌕🌗<br>测试即行为记录<br>回归路径清晰 | 🌕🌕<br>记录碎片化<br>上下文易丢失 |
| 工程化 | 🌕🌕🌕🌕🌗<br>文档/测试/报告齐全<br>目录结构明确 | 🌕🌕🌕🌕<br>产物更少但结构一致<br>工程化仍清晰 | 🌕🌕🌕🌕<br>规范与模板完整<br>实现产物不统一 | 🌕🌕🌕<br>工具链丰富但割裂<br>依赖集成能力 | 🌕🌕🌕🌗<br>测试体系清晰<br>对规范依赖中等 | 🌕🌕<br>产物少且分散<br>缺少流程模板 |
| 自动化 | 🌕🌕🌕🌕<br>脚本化校验+CI 门槛<br>同步率可量化 | 🌕🌕🌕🌕<br>脚本与 CI 可复用<br>校验项减少 | 🌕🌕🌕<br>局部自动生成可行<br>校验链路不统一 | 🌕🌕🌕🌕🌗<br>代理编排自动化强<br>需持续调优 | 🌕🌕🌕<br>测试可自动回归<br>设计与实现依旧手工 | 🌕🌕<br>手工检查为主<br>缺少自动门槛 |
| 人类介入 | 🌕🌕<br>高风险阶段强制人验收<br>人工决策占比较高 | 🌕🌕🌕<br>闸门减少但仍需人审<br>人工决策仍重要 | 🌕🌕🌕<br>规范评审依赖人<br>实现阶段介入中等 | 🌕🌕🌕🌕<br>多代理可替代部分沟通<br>人类主要做监督 | 🌕🌕🌕<br>测试编写与评审依赖人<br>介入中等 | 🌕🌕🌕🌕🌗<br>最少流程约束<br>人工介入最少 |
| 协作 | 🌕🌕🌕🌕🌗<br>产出标准化便交接<br>同步检查降低冲突 | 🌕🌕🌕🌕<br>标准产出仍在<br>沟通成本较低 | 🌕🌕🌕🌕<br>规范对齐提升沟通<br>实现协作依赖执行 | 🌕🌕🌕🌗<br>代理协作替代部分沟通<br>人机协作成本较高 | 🌕🌕🌕🌕<br>测试作为契约对齐<br>协作较顺畅 | 🌕🌕<br>依赖口头与记忆<br>新人接入成本高 |
| 耗时 | 🌕🌕<br>预估 4–10 人时<br>阶段多且有闸门/测试校验 | 🌕🌕🌕🌗<br>预估 2–6 人时<br>阶段减少但保留验证 | 🌕🌕🌕<br>预估 3–8 人时<br>规范编写耗时 | 🌕🌕🌕<br>预估 3–9 人时<br>代理编排与回合开销 | 🌕🌕🌕<br>预估 2–6 人时<br>编写测试带来开销 | 🌕🌕🌕🌕🌗<br>预估 0.5–3 人时<br>直接实现/校验少 |
| token | 🌕🌕<br>预估 200k–600k token<br>成本约 $2–$6<br>文档与验证内容多 | 🌕🌕🌕<br>预估 120k–350k token<br>成本约 $1.2–$3.5<br>阶段减少但仍有文档 | 🌕🌕🌕<br>预估 120k–400k token<br>成本约 $1.2–$4<br>规范文档占比高 | 🌕🌕<br>预估 250k–700k token<br>成本约 $2.5–$7<br>多代理对话开销高 | 🌕🌕🌕🌗<br>预估 80k–250k token<br>成本约 $0.8–$2.5<br>测试与实现多轮 | 🌕🌕🌕🌕🌗<br>预估 20k–80k token<br>成本约 $0.2–$0.8<br>对话最少/产物少 |
| 负担 | 🌕🌕<br>多阶段输出与验收<br>维护投入高 | 🌕🌕🌕<br>文档减少、验证保留<br>维护负担中等 | 🌕🌕🌕<br>规范维护成本中等<br>实现期负担适中 | 🌕🌕🌕<br>运维与提示维护成本<br>稳定性需持续关注 | 🌕🌕🌕<br>测试维护成本持续<br>长期收益中等 | 🌕🌕🌕🌕🌗<br>流程最简<br>负担转移到风险 |

### 优劣对比简述

- **本工作流**：质量与可控性最高，适合复杂需求和团队协作；代价是文档与验证成本高、token 与耗时更大。
- **OpenSpec**：强调规范与一致性，适合“规范先行”的中大型协作；比本工作流更轻，但对实现期验证与回归约束相对弱一些。
- **AutoGen**：多代理协作带来覆盖面与自动化优势，但编排与稳定性成本较高，token/耗时可能上升。
- **TDD**：测试先行提升稳定性与回归效率，适合质量优先场景，但初期编写成本更高。
- **轻量 Prompt 流**：速度快、成本低，适合小修小改或试验；但缺少结构化验证与同步机制，风险外溢更高。

